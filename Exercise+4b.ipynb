{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning VGG on CIFAR-10\n",
    "\n",
    "One of the most common questions we get is how to use neon to load a pre-trained model and fine-tune on a new dataset. In this exercise, we show how to load a pre-trained convolutional neural network (VGG), which was trained on ImageNet, a large corpus of natural images with 1000 categories. We will then use this model to train on the CIFAR-10 dataset, a much smaller set of images with 10 categories.\n",
    "\n",
    "We begin by first generating a computational backend with the `gen_backend` function from neon using a CPU backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend\n",
    "\n",
    "be = gen_backend(batch_size=64, backend='mkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the backend via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<neon.backends.nervanamkl.NervanaMKL object at 0x7fee980c47f0>\n"
     ]
    }
   ],
   "source": [
    "print(be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the VGG Model\n",
    "We begin by first generating our VGG network. [VGG](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) is a popular convolutional neural network with ~19 layers. Not only does this network perform well with fine-tuning, but is also easy to define since the convolutional layers all have 3x3 filter sizes, and only differ in the number of feature maps. \n",
    "\n",
    "We first define some common parameters used by all the convolution layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-caa205d9f272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                \u001b[0;34m'init'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mXavier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                \u001b[0;34m'bias'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                'activation': relu}\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'relu' is not defined"
     ]
    }
   ],
   "source": [
    "from neon.transforms import Rectlin\n",
    "from neon.initializers import Constant, Xavier\n",
    "\n",
    "# define the variable 'relu' to be the Rectlin() activation function\n",
    "...\n",
    "\n",
    "conv_params = {'strides': 1,\n",
    "               'padding': 1,\n",
    "               'init': Xavier(local=True),\n",
    "               'bias': Constant(0),\n",
    "               'activation': relu}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define our network as a list of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-55ad9ef1143b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# set up 3x3 conv stacks with different number of filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvgg_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconv_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mvgg_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconv_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mvgg_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_params' is not defined"
     ]
    }
   ],
   "source": [
    "from neon.layers import Conv, Dropout, Pooling, GeneralizedCost, Affine\n",
    "from neon.initializers import GlorotUniform\n",
    "\n",
    "\n",
    "# Set up the model layers\n",
    "vgg_layers = []\n",
    "\n",
    "# set up 3x3 conv stacks with different number of filters\n",
    "vgg_layers.append(Conv((3, 3, 64), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 64), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 128), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 128), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 256), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 256), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 256), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Affine(nout=4096, init=GlorotUniform(), bias=Constant(0), activation=relu))\n",
    "vgg_layers.append(Dropout(keep=0.5))\n",
    "vgg_layers.append(Affine(nout=4096, init=GlorotUniform(), bias=Constant(0), activation=relu))\n",
    "vgg_layers.append(Dropout(keep=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer of VGG is an `Affine` layer with 1000 units, for the 1000 categories in the ImageNet dataset. However, since our dataset only has 10 classes, we will instead use 10 output units. Create this output layer, and give it the special name 'class_layer' so we know not to load pre-trained weights for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.transforms import Softmax\n",
    "\n",
    "# add an Affine layer to vgg_layers (make sure to use the Softmax activation function):\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to load the pre-trained weights into this model. First we generate a `Model` object to hold the VGG layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.models import Model\n",
    "\n",
    "model = Model(layers=vgg_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained weights\n",
    "Next, we download the pre-trained VGG weights from our [Model Zoo](https://github.com/NervanaSystems/ModelZoo/tree/master/ImageClassification/ILSVRC2012/VGG). Note: this file is quite large (~550MB). By default, the weights file is saved in your home directory. To change this, or if you have already downloaded the file somewhere else, please edit the `filepath` variable below. If file is not there, download it in your browser and then upload it into this workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VGG weights from data/VGG_D.p...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from neon.data.datasets import Dataset\n",
    "from neon.util.persist import load_obj\n",
    "import os\n",
    "\n",
    "# location and size of the VGG weights file\n",
    "url = 'https://s3-us-west-1.amazonaws.com/nervana-modelzoo/VGG'\n",
    "filename = 'VGG_D.p'\n",
    "size = 554227541\n",
    "\n",
    "# edit filepath below if you have the file elsewhere\n",
    "_, filepath = Dataset._valid_path_append('data', '', filename)\n",
    "if not os.path.exists(filepath):\n",
    "    print(\"File {} doesn't exist. Upload it into your notebook and try again.\".format(filepath))\n",
    "\n",
    "# load the weights param file\n",
    "print(\"Loading VGG weights from {}...\".format(filepath))\n",
    "trained_vgg = load_obj(filepath)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In neon, models are saved as python dictionaries. Below are some example calls to explore the model. You can examine the weights, the layer configuration, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has the following keys: dict_keys(['neon_version', 'model', 'epoch_index', 'backend'])\n",
      "The first layer is of type: neon.layers.layer.Convolution\n",
      "The first layer weights have average magnitude of 0.17\n"
     ]
    }
   ],
   "source": [
    "print(\"The dictionary has the following keys: {}\".format(trained_vgg.keys()))\n",
    "\n",
    "layer0 = trained_vgg['model']['config']['layers'][0]\n",
    "print(\"The first layer is of type: {}\".format(layer0['type']))\n",
    "\n",
    "# filter weights of the first layer\n",
    "W = layer0['params']['W']\n",
    "print(\"The first layer weights have average magnitude of {:.2}\".format(abs(W).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to use the below blank code cell to explore the model dictionary! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then iterate over the layers in our model, and load the weights from `trained_vgg` using each layers' `layer.load_weights` method. The final Affine layer is different between our model and the pre-trained model, since the number of classes have changed. Therefore, we break the loop when we encounter the final Affine layer, which has the name `class_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-52c78fe0dc83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparam_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparam_dict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_vgg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_dict_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'class_layer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "param_layers = [l for l in model.layers.layers]\n",
    "param_dict_list = trained_vgg['model']['config']['layers']\n",
    "for layer, params in zip(param_layers, param_dict_list):\n",
    "    if(layer.name == 'class_layer'):\n",
    "        break\n",
    "\n",
    "    # To be sure, we print the name of the layer in our model \n",
    "    # and the name in the vgg model.\n",
    "    print(layer.name + \", \" + params['config']['name'])\n",
    "    layer.load_weights(params, load_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, the above code should have printed out pairs of layer names, from our model and the pre-trained vgg models. The exact name may differ, but the type of layer and layer number should match between the two.\n",
    "\n",
    "## Fine-tuning VGG on the CIFAR-10 dataset\n",
    "Now that we've modified the model for our new CIFAR-10 dataset, and loaded the model weights, let's give training a try!\n",
    "\n",
    "## Aeon Dataloader\n",
    "The CIFAR-10 dataset is small enough to fit into memory, meaning that we would normally use an `ArrayIterator` to generate our dataset. However, the CIFAR-10 images are 28x28, and VGG was trained on ImageNet, which ahs images that are of 224x224 size. For this reason, we use our macrobatching dataloader [aeon](http://aeon.nervanasys.com/), which performs image scaling and cropping on-the-fly. \n",
    "\n",
    "To prepare the data, we first invoke an ingestion script that downloads the data and creates the macrobatches. The script is located in your neon folder. Here we use some python language to extract the path to neon from the virtual environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path to neon as /opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg\n"
     ]
    }
   ],
   "source": [
    "import neon\n",
    "import os\n",
    "neon_path = os.path.split(os.path.dirname(neon.__file__))[0]\n",
    "print(\"Found path to neon as {}\".format(neon_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we execute the ingestion script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/examples/cifar10_msra/data.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%run $neon_path/examples/cifar10_msra/data.py --out_dir data/cifar10/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aeon configuration\n",
    "\n",
    "Aeon allows a diverse set of configurations to specify which transformations are applied on-the-fly during training. These configs are specific as python dictionaries. For more detail, see the [aeon documentation](aeon.nervanasys.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_config = {\n",
    "    'type': 'image',\n",
    "    'height': 224, 'width': 224,\n",
    "    'scale': [0.875, 0.875],\n",
    "    'flip_enable': True\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'manifest_filename': 'data/cifar10/train-index.csv',  # CSV manifest of data\n",
    "    'manifest_root': 'data/cifar10',  # root data directory\n",
    "    'image': {'height': 224, 'width': 224,  # output image size \n",
    "              'scale': [0.875, 0.875],  # random scaling of image before cropping\n",
    "              'flip_enable': True},  # randomly flip image\n",
    "    'type': 'image,label',  # type of data\n",
    "    'minibatch_size': be.bsz,  # batch size\n",
    "    'batch_size': be.bsz,\n",
    "    'etl': [image_config]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use this configuration to create our dataloader. The outputs from the dataloader are then sent through a series of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to create internal loader object: config element {image: \"data/cifar10/train-index.csv\"} is not understood\nconfig is: {\"batch_size\":64,\"etl\":[{\"flip_enable\":true,\"height\":224,\"scale\":[0.875,0.875],\"type\":\"image\",\"width\":224}],\"image\":{\"flip_enable\":true,\"height\":224,\"scale\":[0.875,0.875],\"width\":224},\"manifest_filename\":\"data/cifar10/train-index.csv\",\"manifest_root\":\"data/cifar10\",\"minibatch_size\":64,\"type\":\"image,label\"}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4dc0452e339c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mneon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeCast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBGRMeanSubtract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAeonDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# perform onehot on the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/data/aeon_shim.py\u001b[0m in \u001b[0;36mAeonDataLoader\u001b[0;34m(config, adapter)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mAeonDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoaderAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAeonLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mAeonLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to create internal loader object: config element {image: \"data/cifar10/train-index.csv\"} is not understood\nconfig is: {\"batch_size\":64,\"etl\":[{\"flip_enable\":true,\"height\":224,\"scale\":[0.875,0.875],\"type\":\"image\",\"width\":224}],\"image\":{\"flip_enable\":true,\"height\":224,\"scale\":[0.875,0.875],\"width\":224},\"manifest_filename\":\"data/cifar10/train-index.csv\",\"manifest_root\":\"data/cifar10\",\"minibatch_size\":64,\"type\":\"image,label\"}\n"
     ]
    }
   ],
   "source": [
    "from neon.data.aeon_shim import AeonDataLoader\n",
    "from neon.data.dataloader_transformers import OneHot, TypeCast, BGRMeanSubtract\n",
    "\n",
    "train_set = AeonDataLoader(config, be)\n",
    "\n",
    "train_set = OneHot(train_set, index=1, nclasses=10)  # perform onehot on the labels\n",
    "train_set = TypeCast(train_set, index=0, dtype=np.float32)  # cast the image to float32\n",
    "train_set = BGRMeanSubtract(train_set, index=0)  # subtract image color means (based on default values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimizer configuration\n",
    "\n",
    "For fine-tuning, we want the final `Affine` layer to be updated with a higher learning rate compared to the pre-trained weights throughout the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.optimizers import GradientDescentMomentum, Schedule, MultiOptimizer\n",
    "from neon.transforms import CrossEntropyMulti\n",
    "# define different optimizers for the class_layer and the rest of the network\n",
    "# we use a momentum coefficient of 0.9 and weight decay of 0.0005.\n",
    "opt_vgg = GradientDescentMomentum(0.001, 0.9, wdecay=0.0005)\n",
    "opt_class_layer = GradientDescentMomentum(0.01, 0.9, wdecay=0.0005)\n",
    "\n",
    "# also define optimizers for the bias layers, which have a different learning rate\n",
    "# and not weight decay.\n",
    "opt_bias = GradientDescentMomentum(0.002, 0.9)\n",
    "opt_bias_class = GradientDescentMomentum(0.02, 0.9)\n",
    "\n",
    "# set up the mapping of layers to optimizers\n",
    "opt = MultiOptimizer({'default': opt_vgg, 'Bias': opt_bias,\n",
    "     'class_layer': opt_class_layer, 'class_layer_bias': opt_bias_class})\n",
    "\n",
    "# use cross-entropy cost to train the network\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up callbacks so the model can report progress during training, and then run the `model.fit` function. Note that if you are on a CPU, this next section will take long to finish for you to see results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.callbacks.callbacks import Callbacks\n",
    "\n",
    "# set up callbacks so the model can report progress during training\n",
    "...\n",
    "\n",
    "# call model.fit for ten epochs on the training dataset\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the network cost decrease over the course of training as it fine-tunes on this new dataset.\n",
    "\n",
    "## Customize for your own data\n",
    "With neon it is easy to adapt this notebook to fine-tune on your own dataset -- most of the code above can be reused regardless of your specific problem. There just a few things to do:\n",
    "1. Organize your image dataset into a format that our macrobatching loader recognizes. Run our `batch_writer.py` script to generate the macrobatches. See [Macrobatching](http://neon.nervanasys.com/docs/latest/loading_data.html#macrobatching) in our documentation for more information.\n",
    "2. Modify the number of output units in the last `Affine` layer of the network to match the number of categories in your dataset.\n",
    "\n",
    "Feel free to visit our [github](https://github.com/NervanaSystems/neon/) page or our [documention](http://neon.nervanasys.com/docs/latest/index.html) if you have questions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
